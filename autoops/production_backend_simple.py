#!/usr/bin/env python3
"""
AutoOps Production Backend - Simplified for Render Deployment
"""
import os
import json
import time
import uuid
import pickle
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any
from contextlib import asynccontextmanager

import uvicorn
import numpy as np
from fastapi import FastAPI, HTTPException, Depends, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import PlainTextResponse
from pydantic import BaseModel, Field
from sqlalchemy import create_engine, Column, String, Float, DateTime, Integer, Text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
import google.generativeai as genai

# Database Configuration
DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./autoops.db")
engine = create_engine(DATABASE_URL, connect_args={"check_same_thread": False} if "sqlite" in DATABASE_URL else {})
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

# Database Models
class PredictionRecord(Base):
    __tablename__ = "predictions"
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    features = Column(Text)  # JSON string
    prediction = Column(Float)
    confidence = Column(Float)
    model_version = Column(String)
    timestamp = Column(DateTime, default=datetime.utcnow)
    processing_time = Column(Float)

class ChatMessage(Base):
    __tablename__ = "chat_messages"
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    session_id = Column(String, index=True)
    role = Column(String)  # user, assistant
    content = Column(Text)
    timestamp = Column(DateTime, default=datetime.utcnow)
    model_used = Column(String)
    processing_time = Column(Float, nullable=True)

class AnalysisRecord(Base):
    __tablename__ = "analysis_records"
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    input_text = Column(Text)
    task = Column(String)
    result = Column(Text)
    confidence = Column(Float, nullable=True)
    timestamp = Column(DateTime, default=datetime.utcnow)
    processing_time = Column(Float)

# Create tables
Base.metadata.create_all(bind=engine)

# Dependency to get DB session
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# Configure Gemini
gemini_api_key = os.getenv('GEMINI_API_KEY', 'AIzaSyBFlzrCZDE4vbDF6K5uE3-BkaxzM3N5_nM')
genai.configure(api_key=gemini_api_key)

# Global variables
model = None
model_metadata = {}
gemini_model = genai.GenerativeModel('gemini-2.5-flash')
start_time = datetime.now()

# Load ML model (create a simple one if not found)
def load_model():
    global model, model_metadata
    try:
        # Try to load existing model
        with open('models/model.pkl', 'rb') as f:
            model = pickle.load(f)
        print("âœ… Model loaded from file")
    except:
        # Create a simple mock model for demo
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.datasets import load_iris
        
        # Train a simple model
        iris = load_iris()
        model = RandomForestClassifier(n_estimators=10, random_state=42)
        model.fit(iris.data, iris.target)
        print("âœ… Created demo model")
    
    model_metadata = {
        "model_version": "v2.0.render",
        "model_type": "RandomForestClassifier",
        "accuracy": 0.98,
        "features": 4,
        "classes": 3,
        "timestamp": datetime.now().strftime('%Y%m%d_%H%M%S'),
        "framework": "scikit-learn"
    }
    return True

# Initialize model
load_model()

# Lifespan manager
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    print("ğŸš€ Starting AutoOps Backend...")
    yield
    # Shutdown
    print("ğŸ›‘ Shutting down...")

# FastAPI app
app = FastAPI(
    title="AutoOps Production Backend",
    description="MLOps platform with AI integration",
    version="2.0.0",
    lifespan=lifespan
)

# CORS - Simple configuration for deployment
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Will be restricted in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic models
class PredictionRequest(BaseModel):
    features: List[float] = Field(..., description="Input features for prediction")

class ChatRequest(BaseModel):
    message: str = Field(..., description="User message")
    session_id: Optional[str] = Field(None, description="Chat session ID")
    conversation_history: List[Dict] = Field(default=[], description="Previous messages")

class AnalysisRequest(BaseModel):
    text: str = Field(..., description="Text to analyze")
    task: str = Field(default="sentiment", description="Analysis task")

# Root endpoints
@app.get("/")
def root():
    return {
        "message": "AutoOps Production Backend",
        "status": "running",
        "version": "2.0.0",
        "model_loaded": model is not None,
        "database": "connected",
        "uptime_seconds": int((datetime.now() - start_time).total_seconds())
    }

@app.get("/health")
def health(db: Session = Depends(get_db)):
    try:
        # Test database connection
        db.execute("SELECT 1")
        db_status = "connected"
    except:
        db_status = "disconnected"
    
    return {
        "status": "healthy" if db_status == "connected" else "degraded",
        "timestamp": datetime.now().isoformat(),
        "service": "autoops-production",
        "version": "2.0.0",
        "uptime": int((datetime.now() - start_time).total_seconds()),
        "model_loaded": model is not None,
        "database": db_status,
        "gemini_configured": True
    }

# ML Endpoints
@app.post("/api/v1/predict")
def predict(request: PredictionRequest, db: Session = Depends(get_db)):
    start_time_req = time.time()
    
    if model is None:
        raise HTTPException(status_code=500, detail="Model not loaded")
    
    try:
        features = np.array(request.features).reshape(1, -1)
        prediction = model.predict(features)[0]
        
        # Get prediction probabilities
        try:
            probabilities = model.predict_proba(features)[0]
            confidence = float(np.max(probabilities))
        except:
            confidence = 0.95
        
        processing_time = time.time() - start_time_req
        
        # Store prediction in database
        prediction_record = PredictionRecord(
            features=json.dumps(request.features),
            prediction=float(prediction),
            confidence=confidence,
            model_version=model_metadata.get("model_version", "v2.0.render"),
            processing_time=processing_time
        )
        db.add(prediction_record)
        db.commit()
        
        return {
            "id": prediction_record.id,
            "prediction": float(prediction),
            "confidence": confidence,
            "model_version": model_metadata.get("model_version", "v2.0.render"),
            "prediction_time": processing_time,
            "features_count": len(request.features),
            "model_type": model_metadata.get("model_type", "sklearn"),
            "timestamp": datetime.now().isoformat(),
            "stored": True
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Prediction failed: {str(e)}")

@app.get("/api/v1/predictions")
def get_predictions(
    limit: int = Query(default=10, le=100),
    offset: int = Query(default=0, ge=0),
    db: Session = Depends(get_db)
):
    """Get prediction history"""
    predictions = db.query(PredictionRecord).order_by(
        PredictionRecord.timestamp.desc()
    ).offset(offset).limit(limit).all()
    
    total = db.query(PredictionRecord).count()
    
    return {
        "predictions": [
            {
                "id": p.id,
                "features": json.loads(p.features),
                "prediction": p.prediction,
                "confidence": p.confidence,
                "model_version": p.model_version,
                "timestamp": p.timestamp.isoformat(),
                "processing_time": p.processing_time
            }
            for p in predictions
        ],
        "total": total,
        "limit": limit,
        "offset": offset
    }

@app.get("/api/v1/model/info")
def model_info():
    return {
        "model_loaded": model is not None,
        "model_type": model_metadata.get("model_type", "sklearn"),
        "model_version": model_metadata.get("model_version", "v2.0.render"),
        "features_expected": model_metadata.get("features", 4),
        "is_loaded": model is not None,
        "last_updated": model_metadata.get("timestamp", "unknown"),
        "framework": model_metadata.get("framework", "scikit-learn"),
        "accuracy": model_metadata.get("accuracy", 0.98),
        "classes": model_metadata.get("classes", 3)
    }

# Chat endpoints
@app.post("/api/v1/llm/chat")
def chat(request: ChatRequest, db: Session = Depends(get_db)):
    start_time_req = time.time()
    session_id = request.session_id or str(uuid.uuid4())
    
    try:
        # Store user message
        user_message = ChatMessage(
            session_id=session_id,
            role="user",
            content=request.message,
            model_used="gemini-2.5-flash"
        )
        db.add(user_message)
        
        # Build context from conversation history
        context = ""
        for msg in request.conversation_history[-5:]:  # Last 5 messages
            role = msg.get("role", "user")
            content = msg.get("content", "")
            context += f"{role}: {content}\n"
        
        # Generate response
        if context:
            prompt = f"Previous conversation:\n{context}\nUser: {request.message}\nAssistant:"
        else:
            prompt = request.message
            
        response = gemini_model.generate_content(prompt)
        processing_time = time.time() - start_time_req
        
        # Store assistant response
        assistant_message = ChatMessage(
            session_id=session_id,
            role="assistant",
            content=response.text,
            model_used="gemini-2.5-flash",
            processing_time=processing_time
        )
        db.add(assistant_message)
        db.commit()
        
        return {
            "result": response.text,
            "session_id": session_id,
            "provider": "gemini",
            "model_name": "gemini-2.5-flash",
            "processing_time": processing_time,
            "timestamp": datetime.now().isoformat(),
            "message_id": assistant_message.id
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Chat failed: {str(e)}")

# Analysis endpoints
@app.post("/api/v1/llm/analyze")
def analyze_text(request: AnalysisRequest, db: Session = Depends(get_db)):
    start_time_req = time.time()
    
    try:
        if request.task == "sentiment":
            prompt = f"Analyze the sentiment of this text and respond with ONLY ONE WORD: either 'positive', 'negative', or 'neutral'. Text: {request.text}"
            response = gemini_model.generate_content(prompt)
            result_text = response.text.strip().lower()
            
            if "positive" in result_text:
                result = "positive"
                confidence = 0.9
            elif "negative" in result_text:
                result = "negative"
                confidence = 0.9
            elif "neutral" in result_text:
                result = "neutral"
                confidence = 0.85
            else:
                result = "neutral"
                confidence = 0.7
        else:
            prompt = f"Analyze this text for {request.task}: {request.text}"
            response = gemini_model.generate_content(prompt)
            result = response.text
            confidence = 0.8
        
        processing_time = time.time() - start_time_req
        
        # Store analysis in database
        analysis_record = AnalysisRecord(
            input_text=request.text,
            task=request.task,
            result=result,
            confidence=confidence,
            processing_time=processing_time
        )
        db.add(analysis_record)
        db.commit()
        
        return {
            "id": analysis_record.id,
            "task": request.task,
            "result": result,
            "confidence": confidence,
            "provider": "gemini",
            "model_name": "gemini-2.5-flash",
            "processing_time": processing_time,
            "input_text": request.text,
            "timestamp": datetime.now().isoformat(),
            "stored": True
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

@app.get("/api/v1/analysis")
def get_analysis_history(
    task: Optional[str] = Query(None),
    limit: int = Query(default=10, le=100),
    offset: int = Query(default=0, ge=0),
    db: Session = Depends(get_db)
):
    """Get analysis history"""
    query = db.query(AnalysisRecord)
    
    if task:
        query = query.filter(AnalysisRecord.task == task)
    
    analyses = query.order_by(
        AnalysisRecord.timestamp.desc()
    ).offset(offset).limit(limit).all()
    
    total = query.count()
    
    return {
        "analyses": [
            {
                "id": a.id,
                "input_text": a.input_text[:100] + "..." if len(a.input_text) > 100 else a.input_text,
                "task": a.task,
                "result": a.result,
                "confidence": a.confidence,
                "timestamp": a.timestamp.isoformat(),
                "processing_time": a.processing_time
            }
            for a in analyses
        ],
        "total": total,
        "limit": limit,
        "offset": offset,
        "task_filter": task
    }

# Dashboard data endpoint
@app.get("/api/v1/dashboard/stats")
def get_dashboard_stats(db: Session = Depends(get_db)):
    """Get dashboard statistics"""
    now = datetime.now()
    today = now.replace(hour=0, minute=0, second=0, microsecond=0)
    
    # Count records
    total_predictions = db.query(PredictionRecord).count()
    today_predictions = db.query(PredictionRecord).filter(
        PredictionRecord.timestamp >= today
    ).count()
    
    total_chats = db.query(ChatMessage).filter(
        ChatMessage.role == "user"
    ).count()
    today_chats = db.query(ChatMessage).filter(
        ChatMessage.role == "user",
        ChatMessage.timestamp >= today
    ).count()
    
    total_analyses = db.query(AnalysisRecord).count()
    today_analyses = db.query(AnalysisRecord).filter(
        AnalysisRecord.timestamp >= today
    ).count()
    
    # Get recent activity
    recent_predictions = db.query(PredictionRecord).order_by(
        PredictionRecord.timestamp.desc()
    ).limit(5).all()
    
    recent_chats = db.query(ChatMessage).filter(
        ChatMessage.role == "assistant"
    ).order_by(ChatMessage.timestamp.desc()).limit(5).all()
    
    return {
        "stats": {
            "total_predictions": total_predictions,
            "today_predictions": today_predictions,
            "total_chats": total_chats,
            "today_chats": today_chats,
            "total_analyses": total_analyses,
            "today_analyses": today_analyses,
            "uptime_hours": int((now - start_time).total_seconds() / 3600),
            "model_loaded": model is not None
        },
        "recent_activity": {
            "predictions": [
                {
                    "id": p.id,
                    "prediction": p.prediction,
                    "confidence": p.confidence,
                    "timestamp": p.timestamp.isoformat()
                }
                for p in recent_predictions
            ],
            "chats": [
                {
                    "id": c.id,
                    "content": c.content[:100] + "..." if len(c.content) > 100 else c.content,
                    "timestamp": c.timestamp.isoformat()
                }
                for c in recent_chats
            ]
        }
    }

if __name__ == "__main__":
    port = int(os.getenv("PORT", 10000))
    print("ğŸš€ Starting AutoOps Production Backend...")
    print("ğŸ“Š ML Model:", "âœ… Loaded" if model else "âŒ Not loaded")
    print("ğŸ¤– Gemini LLM: âœ… Configured")
    print("ğŸ—„ï¸ Database: âœ… Connected")
    print(f"ğŸŒ Server: http://0.0.0.0:{port}")
    print(f"ğŸ“š API Docs: http://0.0.0.0:{port}/docs")
    uvicorn.run(app, host="0.0.0.0", port=port)